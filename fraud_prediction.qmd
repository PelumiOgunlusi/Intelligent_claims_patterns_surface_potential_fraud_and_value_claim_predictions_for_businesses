---
author: PELUMI OGUNLUSI
title: "Intelligent claims patterns, surface potential fraud, and value claim predictions for businesses"
format: html
---

# Introduction
This project looks for the best performaing model from 12 classification models to detect fraud cases in a comprehensive collection of insurance claim records.

# Data Understanding
Insurance claims fraud is a deceptive act where someone intentionally provides false or misleading information to an insurance company to obtain financial benefits they are not entitled to. This can involve exaggerating claims, staging accidents, or even faking deaths to collect on life insurance policies. It is a serious crime with legal repercussions, and it ultimately increases the cost of insurance for everyone.

# About this dataset
The dataset, named `insurance_claims.csv`, is a comprehensive collection of insurance claim records. Each row represents an individual claim, and the columns represent various features associated with that claim. 

The dataset is, highlighting features like `months_as_customer`, `age`, `policy_number`, ...etc. The main focus is the `fraud_reported` variable, which indicates claim legitimacy.

Claims data were sourced from various insurance providers, encompassing a diverse array of insurance types including vehicular, property, and personal injury. Each claim's record provides an in-depth look into the individual's background, claim specifics, associated documentation, and feedback from insurance professionals.

The dataset further includes specific indicators and parameters that were considered during the claim's assessment, offering a granular look into the complexities of each claim.

For privacy reasons, and in agreement with the participating insurance providers, certain personal details and specific identifiers have been anonymized. Instead of names or direct identifiers, each entry is associated with a unique ID, ensuring data privacy while retaining data integrity.

The insurance claims were subjected to rigorous examination, encompassing both manual assessments and automated checks. The end result of this examination, specifically whether a claim was deemed fraudulent or not, is clearly indicated for each record.

# Source
[link](https://data.mendeley.com/datasets/992mh7dk9y/2)

# Data Preparation

```{python}
# install and import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import itables
```
## Reading the dataset
```{python}
# Loading the dataset
insurance = pd.read_csv("data/insurance_claims.csv")
```
## Dataset Information
- Inspecting the initial rows, columns, data types, null values, duplicated status, and summary statistics to get an understanding of the dataset's structure.

```{python}
insurance.head()
```

```{python}
insurance.shape
```
- The dataset consists of 1000 rows and 40 columns 

```{python}
insurance.info()
```
- The column titled `_c39` contains no value, hence it'll be dropped

```{python}
insurance.isnull().sum()
```
- The column `authorities_contacted` contains 91 missing values

```{python}
insurance[insurance.duplicated()]
```
- No duplicated data within the dataset 

## Checking Label Proportion
```{python}
insurance['fraud_reported'].value_counts(normalize=True)*100
```

```{python}
# Bar graph of fraud reported cases proportion
fig_bar = px.bar(
    insurance['fraud_reported'].value_counts().reset_index(),
    x='fraud_reported',
    y='count',
    labels={'fraud_reported': 'Fraud Reported', 'count': 'Count'},
    title='Bar Chart of Fraud Reported Cases'
)
fig_bar.show()
```

```{python}
# Pie chart of fraud reported cases proportion
fig_pie = px.pie(
    insurance,
    names='fraud_reported',
    title='Pie Chart of Fraud Reported Cases',
    hole=0.3
)
fig_pie.show()
```
- Significant differences in the proportion of labels in the classification case can cause computers/machines to not learn well so that the model formed can only recognize dominant labels. There is a significant difference in the proportion of labels in this dataset so later we'll have to balance the dataset as we proceed.

# Data cleaning and Preprocessing
```{python}
itables.show(insurance)
```
## Handling Missing Data

```{python}
# dropping the entire column as it contains no values
insurance.drop(columns=['_c39'], inplace=True)
```
```{python}
# dropping only the rows with null values in this column
insurance.dropna(subset=['authorities_contacted'], inplace=True)
```