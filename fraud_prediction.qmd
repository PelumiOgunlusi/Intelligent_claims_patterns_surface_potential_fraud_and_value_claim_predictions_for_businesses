---
author: PELUMI OGUNLUSI
title: "Intelligent claims patterns, surface potential fraud, and value claim predictions for businesses"
format: html
---

# Introduction
This project looks for the best performaing model from 12 classification models to detect fraud cases in a comprehensive collection of insurance claim records.

# Data Understanding
Insurance claims fraud is a deceptive act where someone intentionally provides false or misleading information to an insurance company to obtain financial benefits they are not entitled to. This can involve exaggerating claims, staging accidents, or even faking deaths to collect on life insurance policies. It is a serious crime with legal repercussions, and it ultimately increases the cost of insurance for everyone.

# About this dataset
The dataset, named `insurance_claims.csv`, is a comprehensive collection of insurance claim records. Each row represents an individual claim, and the columns represent various features associated with that claim. 

The dataset is, highlighting features like `months_as_customer`, `age`, `policy_number`, ...etc. The main focus is the `fraud_reported` variable, which indicates claim legitimacy.

Claims data were sourced from various insurance providers, encompassing a diverse array of insurance types including vehicular, property, and personal injury. Each claim's record provides an in-depth look into the individual's background, claim specifics, associated documentation, and feedback from insurance professionals.

The dataset further includes specific indicators and parameters that were considered during the claim's assessment, offering a granular look into the complexities of each claim.

For privacy reasons, and in agreement with the participating insurance providers, certain personal details and specific identifiers have been anonymized. Instead of names or direct identifiers, each entry is associated with a unique ID, ensuring data privacy while retaining data integrity.

The insurance claims were subjected to rigorous examination, encompassing both manual assessments and automated checks. The end result of this examination, specifically whether a claim was deemed fraudulent or not, is clearly indicated for each record.

# Source
[link](https://data.mendeley.com/datasets/992mh7dk9y/2)

# Data Preparation

```{python}
# install and import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import itables
import sklearn
```
## Reading the dataset
```{python}
# Loading the dataset
insurance = pd.read_csv("data/insurance_claims.csv")
```
## Dataset Information
- Inspecting the initial rows, columns, data types, null values, duplicated status, and summary statistics to get an understanding of the dataset's structure.

```{python}
itables.show(insurance)
```

```{python}
insurance.shape
```
- The dataset consists of 1000 rows and 40 columns 

```{python}
insurance.info()
```
- The column titled `_c39` contains no value, hence it'll be dropped

```{python}
insurance.isnull().sum()
```
- The column `authorities_contacted` contains 91 missing values

```{python}
insurance[insurance.duplicated()]
```
- No duplicated data within the dataset 

## Checking Label Proportion
```{python}
insurance['fraud_reported'].value_counts(normalize=True)*100
```

```{python}
# Bar graph of fraud reported cases proportion
fig_bar = px.bar(
    insurance['fraud_reported'].value_counts().reset_index(),
    x='fraud_reported',
    y='count',
    labels={'fraud_reported': 'Fraud Reported', 'count': 'Count'},
    title='Bar Chart of Fraud Reported Cases'
)
fig_bar.show()
```

```{python}
# Pie chart of fraud reported cases proportion
fig_pie = px.pie(
    insurance,
    names='fraud_reported',
    title='Pie Chart of Fraud Reported Cases',
    hole=0.3
)
fig_pie.show()
```
- Significant difference is seen in the proportion of labels in this project case, we'll have to balance the dataset as we proceed.

# Data cleaning and Preprocessing
## Handling Missing Data

```{python}
# dropping the entire column as it contains no values
insurance.drop(columns=['_c39'], inplace=True)
```
```{python}
# dropping only the rows with null values in this column
insurance.dropna(subset=['authorities_contacted'], inplace=True)
```

```{python}
# Find columns containing the '?' symbol
columns_with_question_mark = [col for col in insurance.columns if insurance[col].astype(str).str.contains('\?').any()]
print("Columns containing '?':", columns_with_question_mark)
```
- From just maually viewing the dataset, it was noticed that some columns contain an unknown value '?'. To prevent further reduction of the number of rows in the dataset, I'll be replacing the values in these columns with relevant information that pertains to each column

```{python}
# replacing ? symbol in specific columns with other values
insurance['collision_type'] = insurance['collision_type'].replace('?', 'No Collision')
insurance['property_damage'] = insurance['property_damage'].replace('?', 'Unsure')
insurance['police_report_available'] = insurance['police_report_available'].replace('?', 'In Progress')
```
## Checking for outliers

```{python}
# Plotting the numerical columns in the dataset to check for outliers 
plt.figure(figsize = (20, 15))  
plotnumber = 1  
for col in insurance.columns:  
    if plotnumber <= 24:  
        ax = plt.subplot (5, 5, plotnumber)  
        sns.boxplot (insurance[col])  
        plt.xlabel(col, fontsize = 15)  
        plotnumber += 1  
plt.tight_layout()  
plt.show()  
```
- The only column that contains outliers is the 'umbrella_limit' column which will be adressed later.

# Exploratory Data Analysis
## Descriptive statistics
```{python}
# Summary statistics of all numeric columns in the dataset
insurance.describe()
```

```{python}
# Histogram plot of all numeric columns in the dataset
insurance.hist(figsize=(20,15), color='skyblue')
```
- The highest age is between 30-40 years old. Majority of the customers fall below 250 months as time spent spent so far which the insurance company which is just over 20 years showing majority of these clients are long term customers. There were no significant differences between 'number of witnesses' 

```{python}
# Displot showing relationship between Age and insurance fraud cases
sns.displot(data=insurance, x="age", col='fraud_reported', kde=True)
plt.show()
```
- This shows 30-45 years remain consistent among true and false fraud reported cases 

```{python}
sns.pairplot(data=insurance, hue="fraud_reported",height=3);
plt.show()
```
- This plot helps identify correlations between variables, which is vital in understanding cause-and-effect relationships in data and detect patterns or clusters of data points, aiding in segmentation or classification tasks

```{python}
# Correlation plot of numerical columns in the dataset
plt.figure(figsize = (10,6))  
numeric_df = insurance.select_dtypes(include=np.number)
corr = numeric_df.corr()  
sns.heatmap(data = corr, annot = True, fmt = '.2g', linewidth = 1)  
plt.show()  
```
```{python}
# Checking for multicollinearity between features
plt.figure(figsize = (30, 25))    
corr = numeric_df.corr()  
mask = np.triu(np.ones_like(corr, dtype = bool))    
sns.heatmap(data = corr, mask = mask, annot = True, fmt = '.2g', linewidth = 1)  
plt.show()  
```

```{python}
# Visualizing feature distributions for fraudulent vs non-fraudulent claims
categorical_cols = insurance.select_dtypes(include='object').columns.drop('fraud_reported')
for col in categorical_cols:
    plt.figure(figsize=(8,4))
    sns.countplot(data=insurance, x=col, hue='fraud_reported')
    plt.title(f'Distribution of {col} by Fraud Reported')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
```

```{python}
# Visualizing numerical features
numerical_cols = insurance.select_dtypes(include=np.number).columns
for col in numerical_cols:
    plt.figure(figsize=(8,4))
    sns.kdeplot(data=insurance, x=col, hue='fraud_reported', fill=True)
    plt.title(f'Distribution of {col} by Fraud Reported')
    plt.tight_layout()
    plt.show()
```
- EDA doesn't really provide accurate information on patterns between the features and the target column due to the imbalanced nature of the dataset. We will rectify this as we proceed
    
# Feature Engineering
## Separating the feature and target column
```{python}
x = insurance.drop('fraud_reported', axis = 1)  
y = insurance['fraud_reported']  
```
## Converting columns `policy_bind_date` and `incident_date` to date_time datatype and extracting the year columns from both of these  

```{python}
# Convert 'policy_bind_date' and 'incident_date' to datetime
x['policy_bind_date'] = pd.to_datetime(x['policy_bind_date'])
x['incident_date'] = pd.to_datetime(x['incident_date'])

# Extract year from both columns
x['policy_bind_year'] = x['policy_bind_date'].dt.year
x['incident_year'] = x['incident_date'].dt.year
x.drop(columns=['policy_bind_date','incident_date'], inplace=True)
x
``` 

## Encoding Categorical Columns
```{python}
# Extracting categorical columns
cat_col = x.select_dtypes(include='object')
cat_col = cat
```

```{python}
# Selecting columns of interest in the categorical columns; only columns with less than 10 unique values will be used for the model 
to_drop =  ['insured_occupation','insured_hobbies','incident_location','auto_model']
cat_col.drop(columns=to_drop, inplace=True)
cat_col
```
```{python}
# Encoding all categorical variables using one-hot encoding
cat_col_encoded = pd.get_dummies(cat_col, drop_first=True)
cat_col_encoded
```
## Dealing with outliers in numeric dataset
```{python}
# Extracting numerical columns
num_col = x.select_dtypes(include=np.number)
num_col
```
```{python}
# Normalization of the column with outlier 
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
num_col['umbrella_limit'] = scaler.fit_transform(num_col['umbrella_limit'].values.reshape(-1, 1))
```
## Concatenating both numerical and categorical columns to just one dataset
```{python}
# Concatenate numerical and encoded categorical columns
x_processed = pd.concat([num_col, cat_col_encoded], axis=1)
x_processed
```

## Encoding the target column
```{python}
# Encode target variable: 0 for 'N', 1 for 'Y'
y = y.map({'N': 0, 'Y': 1})
y
```

## Balancing the dataset using oversampling SMOTE
```{python}
## imbalance dataset handling and then we can feed our data to the model
from imblearn.over_sampling import SMOTE
smote = SMOTE()
x_sm, y_sm = smote.fit_resample(x_processed,y)
oversample_plot = y_sm.value_counts().reset_index()
oversample_plot.columns = ['Labels', 'fraud_reported']
print (oversample_plot)
sns.barplot (x='Labels',y='fraud_reported', data=oversample_plot);
plt.title('status after upsampling');
```

## Feature selection
- Here, we use an embedding method to select the most important features in the entire dataset.  Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. [for further information](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/#h-embedded-methods) 
```{python}
from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(x_sm,y_sm)
```

```{python}
# PLot showing the top 18 features that can be used for model training
plt.figure(figsize=(10,10))
feat = pd.Series(model.feature_importances_,index= x_sm.columns)
feat.nlargest(18).plot(kind='bar')
```

```{python}
# Selecting only the top 18 columns for modelling
plot = feat.nlargest(18)
plot.index
cols=['incident_severity_Minor Damage', 'incident_severity_Total Loss',
       'collision_type_Rear Collision', 'property_damage_Unsure',
       'insured_sex_MALE', 'policy_csl_250/500', 'authorities_contacted_Other',
       'policy_state_OH', 'incident_state_SC',
       'incident_type_Single Vehicle Collision', 'insured_zip',
       'property_damage_YES', 'vehicle_claim', 'policy_state_IN',
       'policy_number', 'total_claim_amount', 'bodily_injuries',
       'policy_csl_500/1000']
x_new = x_sm[cols]
```

# Modeling:
```{python}
# importing required libraries
from sklearn.model_selection import train_test_split
```

```{python}
# Spliting the dataset into training and test sets to ensure the model's generalizability.
x_train,x_test,y_train,y_test = train_test_split(x_new,y_sm,test_size = 0.2,random_state=4)
```
## Models to try
1. Support Vector Classifier
2. KNN
3. Decision Tree Classifier
4. Random Forest Classifier
5. Ada Boost Classifier
6. Gradient Boosting Classifier
7. Stochastic Gradient Boosting (SGB)
8. XgBoost
9. Cat Boost Classifier
10. Extra Trees Classifier
11. LGBM Classifier
12. Voting Classifier

